{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np \n",
    "import math\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "#cart-pole environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "class CartPole():\n",
    "\tdef __init__(self, buckets = (1, 1, 6, 3,), n_episodes = 1000, solved_t = 195,\n",
    "\t\t\t\tmin_epsilon = 0.1, min_alpha = 0.1, gamma = 0.99):\n",
    "\t\tself.buckets = buckets  #discrete values for each feature space dimension\n",
    "\t\t\t\t\t\t\t\t#(position, velocity, angle, angular velocity)\n",
    "\t\t\n",
    "        self.n_episodes = n_episodes  \n",
    "\t\t\n",
    "        self.min_alpha = min_alpha\n",
    "\t\t\n",
    "        self.min_epsilon = min_epsilon\n",
    "\t\t\n",
    "        self.gamma = gamma  #discount factor to determine importance of reward\n",
    "        \n",
    "\t\tself.solved_t = solved_t  #lower bound before episode ends\n",
    "\n",
    "\t\tself.Q_table = np.zeros(self.buckets + (env.action_space.n, ))  #action space (left, right)\n",
    "\n",
    "\n",
    "\tdef discretize_state(self, state):\n",
    "\t\tupper_bounds = env.observation_space.high\t#upper and lower bounds of state dimensions\n",
    "\t\tlower_bounds = env.observation_space.low\n",
    "\n",
    "\t\tupper_bounds[1] = 0.5\n",
    "\t\tupper_bounds[3] = math.radians(50)  #setting manual bounds for velocity and angluar velocity\n",
    "\t\tlower_bounds[1] = -0.5\n",
    "\t\tlower_bounds[3] = -math.radians(50)\n",
    " \t\t\n",
    " \t\t#discretizing each input dimension into one of the buckets\n",
    "\t\twidth = [upper_bounds[i] - lower_bounds[i] for i in range(len(state))]\n",
    "\t\tratios = [(state[i] - lower_bounds[i]) / width[i] for i in range(len(state))]\n",
    "\t\tbucket_indices = [int(round(ratios[i] * (self.buckets[i] - 1))) for i in range(len(state))]\n",
    "\n",
    "\t\t#making the range of indices to [0, bucket_length]\n",
    "\t\tbucket_indices = [max(0, min(bucket_indices[i], self.buckets[i] - 1)) for i in range(len(state))]\n",
    "\n",
    "\t\treturn tuple(bucket_indices)\n",
    "\n",
    "\n",
    "\tdef select_action(self, state, epsilon):\n",
    "\t\t#implement the epsilon-greedy approach\n",
    "\t\tif random.random() <= epsilon:\n",
    "\t\t\treturn env.action_space.sample()  #sample a random action with probability epsilon\n",
    "\t\telse:\n",
    "\t\t\treturn np.argmax(self.Q_table[state])  #choose greedy action with hightest Q-value\n",
    "\n",
    "#choose decaying epsilon in range [min_epsilon, 1]\n",
    "\tdef get_epsilon(self, episode_number):\n",
    "\t\t\n",
    "\t\treturn max(self.min_epsilon, min(1, 1 - math.log10((episode_number + 1) / 25)))\n",
    "\n",
    "#choose decaying alpha in range [min_alpha, 1]\n",
    "\tdef get_alpha(self, episode_number): \n",
    "\t\t\n",
    "\t\treturn max(self.min_alpha, min(1, 1 - math.log10((episode_number + 1) / 25)))\n",
    "\n",
    "#updates the state-action pairs based on future reward\n",
    "\tdef update_table(self, old_state, action, reward, new_state, alpha):\n",
    "\t\t\n",
    "\t\tnew_state_Q_value = np.max(self.Q_table[new_state])\n",
    "\t\tself.Q_table[old_state][action] += alpha * (reward + self.gamma * new_state_Q_value - self.Q_table[old_state][action])\n",
    "\n",
    "\n",
    "\tdef run(self):\n",
    "\t\t#runs episodes\n",
    "\t\tscores = deque(maxlen = 100)\n",
    "\n",
    "\t\tfor episode in range(self.n_episodes):\n",
    "\t\t\tdone = False\n",
    "\t\t\talpha = self.get_alpha(episode)\n",
    "\t\t\tepsilon = self.get_epsilon(episode)\n",
    "\t\t\tepisode_reward = 0\n",
    "\n",
    "\t\t\tobs = env.reset()\n",
    "\t\t\tcurr_state = self.discretize_state(obs)\n",
    "\n",
    "\t\t\twhile not done:\n",
    "\t\t\t\tenv.render()\n",
    "\t\t\t\taction = self.select_action(curr_state, epsilon)\n",
    "\t\t\t\tobs, reward, done, info = env.step(action)\n",
    "\t\t\t\tnew_state = self.discretize_state(obs)\n",
    "\n",
    "\t\t\t\tself.update_table(curr_state, action, reward, new_state, alpha)\n",
    "\t\t\t\tcurr_state = new_state\n",
    "\t\t\t\tepisode_reward += reward\n",
    "\n",
    "\t\t\tscores.append(episode_reward)\n",
    "\t\t\tmean_reward = np.mean(scores)\n",
    "\n",
    "\t\t\tif mean_reward > self.solved_t and (episode + 1) >= 100:\n",
    "\t\t\t\tprint(\"Ran {} episodes, solved after {} trials\".format(episode + 1, episode + 1 - 100))\n",
    "\t\t\t\treturn episode + 1 - 100\n",
    "\t\t\telif (episode + 1) % 50 == 0 and(episode + 1) >= 100:\n",
    "\t\t\t\tprint(\"Episode number: {}, mean reward over past 100 episodes is {}\".format(episode + 1, mean_reward)) \n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"Episode {}, reward {}\".format(episode + 1, episode_reward))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tcartpole = CartPole()\n",
    "\tcartpole.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
